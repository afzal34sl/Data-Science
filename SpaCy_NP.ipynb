{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afzal34sl/Data-Science/blob/ML/SpaCy_NP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Extracting Noun Pharses with Adjectives where Nouns are singular**\n",
        "We will use SpaCy library to achive this task and create a custom Noun Chunker where it can extract multiple adjectives for a single noun without punctuation and conjuctions. \n",
        "The Noun Chunker can be changed to meet different needs."
      ],
      "metadata": {
        "id": "cuIrIX7uELOi"
      },
      "id": "cuIrIX7uELOi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install all the libraries, if necessary"
      ],
      "metadata": {
        "id": "sQQTUolUFZwO"
      },
      "id": "sQQTUolUFZwO"
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install spacy #Main Library\n",
        "# ! pip install watermark\n",
        "# ! pip install contractions\n",
        "# ! pip install numpy\n",
        "# ! pip install pandas\n",
        "# ! python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "ULCN253FFuoP"
      },
      "id": "ULCN253FFuoP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all the files"
      ],
      "metadata": {
        "id": "qn-3W9U1Emu4"
      },
      "id": "qn-3W9U1Emu4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2027dbf8",
      "metadata": {
        "id": "2027dbf8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import contractions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Watermark\n",
        "\n",
        "This library specifies what are the specifications of the computer used and library versions"
      ],
      "metadata": {
        "id": "WWzVlj6gYvLE"
      },
      "id": "WWzVlj6gYvLE"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark -a \"Afzal Azeem Chowdhary\" -u -d -v -m --iversions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7oxlI76ZOlz",
        "outputId": "ad1587a2-4814-46d6-d332-db53f64976e4"
      },
      "id": "H7oxlI76ZOlz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Afzal Azeem Chowdhary\n",
            "\n",
            "Last updated: 2022-07-07\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.13\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.188+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "contractions: 0.1.72\n",
            "spacy       : 3.3.1\n",
            "pandas      : 1.3.5\n",
            "IPython     : 5.5.0\n",
            "numpy       : 1.21.6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Input Text\n",
        "\n",
        "For simplicity, we will be only assigning the text to a variable called 'txt'.\n",
        "\n",
        "We will first covert the whole  text to lowercase for getting consistent singular nouns. If not then this will happen in the example text. \n",
        "For e.g., Ramen will remain as Ramen but ramen will be converted to raman (singular of ramen). We can skip this step, if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "t7B-6rhrJhDq"
      },
      "id": "t7B-6rhrJhDq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a038bcf",
      "metadata": {
        "id": "4a038bcf"
      },
      "outputs": [],
      "source": [
        "txt = str(\"\"\"An apple a day keeps the doctor away. The doctor lives with his intelligent wife and their handsome son named Ramon. Ramon doesn't eat apples but he eats Ramen. Although Ramon agrees that green apples are better than hot, vibrant and spicy ramen.\"\"\")\n",
        "# txt = str(\"\"\"An apple a day keeps the doctor away. The doctor lives with his wife and their son named Ramon. Ramon doesn't eat apples but he eats Ramen. Although Ramon agrees that green apples are better than hot ramen.\"\"\")\n",
        "txt = str(\"\"\"A shocking video of a driver losing control on a wet road and smashing into another vehicle has surfaced online. \n",
        "The short clip was shared on Reddit. The caption of the post informed that the incident took place in Kasaragod in Kerala. The terrifying video shows a speeding car losing control on a wet road. The driver who recorded the incident was driving smoothly when suddenly a red car coming from the opposite direction rammed into it. \"\"\")\n",
        "txt = txt.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Contractions\n",
        "Also, we can use the contractions library to convert words like 'doesn't' to 'does not', 'isn't' to 'is not', etc. Although, SpaCy is robust to handle such situations. We can skip this step, if needed.\n"
      ],
      "metadata": {
        "id": "2kJosaP3MDAE"
      },
      "id": "2kJosaP3MDAE"
    },
    {
      "cell_type": "code",
      "source": [
        "# txt = contractions.fix(txt)"
      ],
      "metadata": {
        "id": "uwZRHxymMP3D"
      },
      "id": "uwZRHxymMP3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Formatting\n",
        "\n",
        "Create a dataframe and split based on fullstop.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1EXet32XJ_0f"
      },
      "id": "1EXet32XJ_0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8e1243",
      "metadata": {
        "id": "0b8e1243"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['sentence'])\n",
        "df['sentence'] = ([x for x in txt.split('.')])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert each dataframe cell to string\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Hj3kFMlM1rH"
      },
      "id": "3Hj3kFMlM1rH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a27e441",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a27e441",
        "outputId": "fb4657bc-de47-43f7-bd70-b578c8ce060b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    a shocking video of a driver losing control on...\n",
            "1                  the short clip was shared on reddit\n",
            "2     the caption of the post informed that the inc...\n",
            "3     the terrifying video shows a speeding car los...\n",
            "4     the driver who recorded the incident was driv...\n",
            "5                                                     \n",
            "Name: sentence, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df['sentence'].astype(str))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Corpus"
      ],
      "metadata": {
        "id": "wM-y5EUiO848"
      },
      "id": "wM-y5EUiO848"
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg') #The last two characters can be changed to sm and md based on requirements."
      ],
      "metadata": {
        "id": "9-zBnlDvPERB"
      },
      "id": "9-zBnlDvPERB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS Tagging\n",
        "\n",
        "We will iterate each sentence and get the tags.\n",
        "\n",
        "Created a function that first uused to the SpaCy corpus to get the "
      ],
      "metadata": {
        "id": "26LtfJYFODO_"
      },
      "id": "26LtfJYFODO_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1ddbb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa1ddbb8",
        "outputId": "9a5aaa18-b053-4ef0-db7e-5a74bc6b2489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "a          a          DET      DT       det      x               1        1\n",
            "shocking   shocking   ADJ      JJ       amod     xxxx            1        0\n",
            "video      video      NOUN     NN       nsubj    xxxx            1        0\n",
            "of         of         ADP      IN       prep     xx              1        1\n",
            "a          a          DET      DT       det      x               1        1\n",
            "driver     driver     NOUN     NN       npadvmod xxxx            1        0\n",
            "losing     lose       VERB     VBG      amod     xxxx            1        0\n",
            "control    control    NOUN     NN       pobj     xxxx            1        0\n",
            "on         on         ADP      IN       prep     xx              1        1\n",
            "a          a          DET      DT       det      x               1        1\n",
            "wet        wet        ADJ      JJ       amod     xxx             1        0\n",
            "road       road       NOUN     NN       pobj     xxxx            1        0\n",
            "and        and        CCONJ    CC       cc       xxx             1        1\n",
            "smashing   smash      VERB     VBG      conj     xxxx            1        0\n",
            "into       into       ADP      IN       prep     xxxx            1        1\n",
            "another    another    DET      DT       det      xxxx            1        1\n",
            "vehicle    vehicle    NOUN     NN       pobj     xxxx            1        0\n",
            "has        have       AUX      VBZ      aux      xxx             1        1\n",
            "surfaced   surface    VERB     VBN      ROOT     xxxx            1        0\n",
            "online     online     ADV      RB       advmod   xxxx            1        0\n",
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "                      SPACE    _SP      dep                      0        0\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "short      short      ADJ      JJ       amod     xxxx            1        0\n",
            "clip       clip       NOUN     NN       nsubjpass xxxx            1        0\n",
            "was        be         AUX      VBD      auxpass  xxx             1        1\n",
            "shared     share      VERB     VBN      ROOT     xxxx            1        0\n",
            "on         on         ADP      IN       prep     xx              1        1\n",
            "reddit     reddit     NOUN     NN       pobj     xxxx            1        0\n",
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "                      SPACE    _SP      dep                      0        0\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "caption    caption    NOUN     NN       appos    xxxx            1        0\n",
            "of         of         ADP      IN       prep     xx              1        1\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "post       post       NOUN     NN       pobj     xxxx            1        0\n",
            "informed   inform     VERB     VBN      ROOT     xxxx            1        0\n",
            "that       that       SCONJ    IN       mark     xxxx            1        1\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "incident   incident   NOUN     NN       nsubj    xxxx            1        0\n",
            "took       take       VERB     VBD      ccomp    xxxx            1        0\n",
            "place      place      NOUN     NN       dobj     xxxx            1        0\n",
            "in         in         ADP      IN       prep     xx              1        1\n",
            "kasaragod  kasaragod  PROPN    NNP      pobj     xxxx            1        0\n",
            "in         in         ADP      IN       prep     xx              1        1\n",
            "kerala     kerala     PROPN    NNP      pobj     xxxx            1        0\n",
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "                      SPACE    _SP      dep                      0        0\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "terrifying terrifying ADJ      JJ       amod     xxxx            1        0\n",
            "video      video      NOUN     NN       nsubj    xxxx            1        0\n",
            "shows      show       VERB     VBZ      ROOT     xxxx            1        0\n",
            "a          a          DET      DT       det      x               1        1\n",
            "speeding   speed      VERB     VBG      amod     xxxx            1        0\n",
            "car        car        NOUN     NN       nsubj    xxx             1        0\n",
            "losing     lose       VERB     VBG      ccomp    xxxx            1        0\n",
            "control    control    NOUN     NN       dobj     xxxx            1        0\n",
            "on         on         ADP      IN       prep     xx              1        1\n",
            "a          a          DET      DT       det      x               1        1\n",
            "wet        wet        ADJ      JJ       amod     xxx             1        0\n",
            "road       road       NOUN     NN       pobj     xxxx            1        0\n",
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "                      SPACE    _SP      dep                      0        0\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "driver     driver     NOUN     NN       nsubj    xxxx            1        0\n",
            "who        who        PRON     WP       nsubj    xxx             1        1\n",
            "recorded   record     VERB     VBD      relcl    xxxx            1        0\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "incident   incident   NOUN     NN       dobj     xxxx            1        0\n",
            "was        be         AUX      VBD      aux      xxx             1        1\n",
            "driving    drive      VERB     VBG      ROOT     xxxx            1        0\n",
            "smoothly   smoothly   ADV      RB       advmod   xxxx            1        0\n",
            "when       when       SCONJ    WRB      advmod   xxxx            1        1\n",
            "suddenly   suddenly   ADV      RB       advmod   xxxx            1        0\n",
            "a          a          DET      DT       det      x               1        1\n",
            "red        red        ADJ      JJ       amod     xxx             1        0\n",
            "car        car        NOUN     NN       nsubj    xxx             1        0\n",
            "coming     come       VERB     VBG      acl      xxxx            1        0\n",
            "from       from       ADP      IN       prep     xxxx            1        1\n",
            "the        the        DET      DT       det      xxx             1        1\n",
            "opposite   opposite   ADJ      JJ       amod     xxxx            1        0\n",
            "direction  direction  NOUN     NN       pobj     xxxx            1        0\n",
            "rammed     ram        VERB     VBD      advcl    xxxx            1        0\n",
            "into       into       ADP      IN       prep     xxxx            1        1\n",
            "it         it         PRON     PRP      pobj     xx              1        1\n",
            "text       lemma_     pos_     tag_     dep_     shape_   is_alpha is_stop \n",
            "                      SPACE    _SP      dep                      0        0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [(a, DT), (shocking, JJ), (video, NN), (of, IN...\n",
              "1    [( , _SP), (the, DT), (short, JJ), (clip, NN),...\n",
              "2    [( , _SP), (the, DT), (caption, NN), (of, IN),...\n",
              "3    [( , _SP), (the, DT), (terrifying, JJ), (video...\n",
              "4    [( , _SP), (the, DT), (driver, NN), (who, WP),...\n",
              "5                                           [( , _SP)]\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "def onegram(text):\n",
        "    doc = nlp(text) #SpaCy does need to be explicitly told to first tokenize the sentences than the words and later getting the tags.\n",
        "    result = []\n",
        "    print(\"{0:10} {1:10} {2:8} {3:8} {4:8} {5:8} {6:8} {7:8}\".format(\"text\", \"lemma_\", \"pos_\", \"tag_\", \"dep_\",\n",
        "            \"shape_\", \"is_alpha\", \"is_stop\"))\n",
        "\n",
        "    for token in doc:\n",
        "        print(\"{0:10} {1:10} {2:8} {3:8} {4:8} {5:8} {6:8} {7:8}\".format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop))\n",
        "        result.append((token.text, token.tag_))\n",
        "    return result\n",
        "df['sentence'].apply(onegram)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Noun Chunker"
      ],
      "metadata": {
        "id": "HlD__TUPPY6J"
      },
      "id": "HlD__TUPPY6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inbuild function \"noun_chunks\" will get all the Noun Phrases (NPs). But, it will miss Proper Nouns like 'day' in the example text. Since, our requirements are different we will create a custom noun chunker or parser.\n",
        "\n",
        "The custom noun chunker is 'extractNP' which calls all a another recursive function called 'check_children' which get all the Adjectives of a Noun as required in our case. Since, a Tree structure  is created during POS Tagging, the noun will have its adjectives, conjuction and punctuation as its children. Therefore, we loop through them to get all the adjectives of a noun."
      ],
      "metadata": {
        "id": "-5wyYLivPwV3"
      },
      "id": "-5wyYLivPwV3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9ba527",
      "metadata": {
        "id": "3c9ba527"
      },
      "outputs": [],
      "source": [
        "def check_children(token):\n",
        "    chunk = ''\n",
        "    for w in token.children:\n",
        "        # print(w.text)\n",
        "        if w.pos_ == 'ADJ':# or w.pos_ == 'CCONJ': #or w.pos_ == 'PUNCT'\n",
        "            chunk = chunk + w.text + ' ' + check_children(w)\n",
        "            \n",
        "    return chunk\n",
        "    \n",
        "def extractNP(text):\n",
        "    doc = nlp(text)\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "            chunk = ''\n",
        "            chunk = check_children(token) + token.lemma_\n",
        "            if chunk != '':\n",
        "#               print(chunk)\n",
        "                result.append(chunk)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the result after converting back to list"
      ],
      "metadata": {
        "id": "6jtVJU5xRIRq"
      },
      "id": "6jtVJU5xRIRq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c76b664",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c76b664",
        "outputId": "c5538a31-bbcc-4f87-a2cc-67d967ea4caa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['shocking video', 'driver', 'control', 'wet road', 'vehicle'],\n",
              " ['short clip', 'reddit'],\n",
              " ['caption', 'post', 'incident', 'place', 'kasaragod', 'kerala'],\n",
              " ['terrifying video', 'car', 'control', 'wet road'],\n",
              " ['driver', 'incident', 'red car', 'opposite direction'],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df['sentence'].apply(extractNP).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All the code is loaded into a single cell to run once expect the text cell.**"
      ],
      "metadata": {
        "id": "PrqIvD4QZi07"
      },
      "id": "PrqIvD4QZi07"
    },
    {
      "cell_type": "code",
      "source": [
        "txt = str(\"\"\"An apple a day keeps the doctor away. The doctor lives with his intelligent wife and their handsome son named Ramon. Ramon doesn't eat apples but he eats Ramen. Although Ramon agrees that green apples are better than hot, vibrant and spicy ramen.\"\"\")\n",
        "# txt = str(\"\"\"An apple a day keeps the doctor away. The doctor lives with his wife and their son named Ramon. Ramon doesn't eat apples but he eats Ramen. Although Ramon agrees that green apples are better than hot ramen.\"\"\")\n",
        "txt = str(\"\"\"A shocking video of a driver losing control on a wet road and smashing into another vehicle has surfaced online. \n",
        "The short clip was shared on Reddit. The caption of the post informed that the incident took place in Kasaragod in Kerala. The terrifying video shows a speeding car losing control on a wet road. The driver who recorded the incident was driving smoothly when suddenly a red car coming from the opposite direction rammed into it. \"\"\")\n",
        "# txt = txt.replace(\"\\n\",\"\")\n",
        "txt = txt.lower()"
      ],
      "metadata": {
        "id": "2ba6DVDabVHs"
      },
      "id": "2ba6DVDabVHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install spacy #Main Library\n",
        "# ! pip install watermark\n",
        "# ! pip install contractions\n",
        "# ! pip install numpy\n",
        "# ! pip install pandas\n",
        "# ! python -m spacy download en_core_web_lg\n",
        "\n",
        "#Importing and Loading libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import contractions\n",
        "\n",
        "%load_ext watermark\n",
        "%watermark -a \"Afzal Azeem Chowdhary\" -u -d -v -m --iversions\n",
        "\n",
        "#Working with Data\n",
        "df = pd.DataFrame(columns=['sentence'])\n",
        "df['sentence'] = ([x for x in txt.split('.')])\n",
        "\n",
        "df['sentence'].astype(str)\n",
        "\n",
        "#Loading Scorpus\n",
        "nlp = spacy.load('en_core_web_lg') #The last two characters can be changed to sm and md based on requirements, but they need to be installed as well.\n",
        "\n",
        "#POS Tagging\n",
        "def onegram(text):\n",
        "    doc = nlp(text) #SpaCy does need to be explicitly told to first tokenize the sentences than the words and later getting the tags.\n",
        "    result = []\n",
        "    # print(\"{0:10} {1:10} {2:8} {3:8} {4:8} {5:8} {6:8} {7:8}\".format(\"text\", \"lemma_\", \"pos_\", \"tag_\", \"dep_\",\n",
        "    #         \"shape_\", \"is_alpha\", \"is_stop\"))\n",
        "\n",
        "    # for token in doc:\n",
        "    #     print(\"{0:10} {1:10} {2:8} {3:8} {4:8} {5:8} {6:8} {7:8}\".format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "    #         token.shape_, token.is_alpha, token.is_stop))\n",
        "    #     result.append((token.text, token.tag_))\n",
        "    return result\n",
        "\n",
        "df['sentence'].apply(onegram)\n",
        "\n",
        "\n",
        "##Chunking NP's\n",
        "def check_children(token):\n",
        "    chunk = ''\n",
        "    for w in token.children:\n",
        "        # print(w.text)\n",
        "        if w.pos_ == 'ADJ':# or w.pos_ == 'CCONJ': #or w.pos_ == 'PUNCT'\n",
        "            chunk = chunk + w.text + ' ' + check_children(w)\n",
        "            \n",
        "    return chunk\n",
        "    \n",
        "def extractNP(text):\n",
        "    doc = nlp(text)\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "            chunk = ''\n",
        "            chunk = check_children(token) + token.lemma_\n",
        "            if chunk != '':\n",
        "#               print(chunk)\n",
        "                result.append(chunk)\n",
        "    return result\n",
        "\n",
        "#Displaying Result\n",
        "df['sentence'].apply(extractNP).tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB1kFS0fa6zs",
        "outputId": "b57c6b81-b2a8-42c9-d092-79563acc484e"
      },
      "id": "WB1kFS0fa6zs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The watermark extension is already loaded. To reload it, use:\n",
            "  %reload_ext watermark\n",
            "Author: Afzal Azeem Chowdhary\n",
            "\n",
            "Last updated: 2022-07-07\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.13\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.188+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "contractions: 0.1.72\n",
            "spacy       : 3.3.1\n",
            "pandas      : 1.3.5\n",
            "IPython     : 5.5.0\n",
            "numpy       : 1.21.6\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['shocking video', 'driver', 'control', 'wet road', 'vehicle'],\n",
              " ['short clip', 'reddit'],\n",
              " ['caption', 'post', 'incident', 'place', 'kasaragod', 'kerala'],\n",
              " ['terrifying video', 'car', 'control', 'wet road'],\n",
              " ['driver', 'incident', 'red car', 'opposite direction'],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "SpaCy NP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sQQTUolUFZwO",
        "qn-3W9U1Emu4",
        "WWzVlj6gYvLE",
        "t7B-6rhrJhDq",
        "26LtfJYFODO_"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}